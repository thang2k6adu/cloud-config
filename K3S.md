# üöÄ H∆Ø·ªöNG D·∫™N TRI·ªÇN KHAI K3S CLUSTER (MASTER + WORKER)

---

## B∆Ø·ªöC 1: ƒê·ªîI HOSTNAME (TR√äN NODE MASTER)

> Nh·ªõ d√πng `ip a` ƒë·ªÉ check **IP / mask / gateway** v√† thay cho ƒë√∫ng tr∆∞·ªõc khi l√†m b·∫•t c·ª© ƒëi·ªÅu g√¨.

```bash
sudo hostnamectl set-hostname k3s-master
sudo nano /etc/hosts
```

V√≠ d·ª• n·ªôi dung:

```txt
127.0.0.1 localhost
192.168.0.104 k3s-master
```

Reboot:

```bash
sudo reboot
```

---

## B∆Ø·ªöC 2: SET IP Tƒ®NH + DISABLE CLOUD-INIT (MASTER)

Disable cloud-init network:

```bash
sudo nano /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
```

N·ªôi dung:

```yaml
network: {config: disabled}
```

X√≥a netplan c≈©:

```bash
sudo rm -f /etc/netplan/50-cloud-init.yaml
```

T·∫°o netplan m·ªõi:

```bash
sudo nano /etc/netplan/01-static.yaml
```

N·ªôi dung:

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    ens33:
      dhcp4: no
      addresses:
        - 192.168.0.104/24
      gateway4: 192.168.0.1
      nameservers:
        addresses:
          - 8.8.8.8
          - 1.1.1.1
```

Apply:

```bash
sudo netplan apply
```

Check IP:

```bash
ip a
```

---

## B∆Ø·ªöC 3: SCAN IP C√ÅC SERVER WORKER (TR√äN MASTER)

C√†i `nmap`:

```bash
sudo apt install nmap -y
```

Auto generate inventory file

> ‚ö†Ô∏è Nh·ªõ s·ª≠a subnet + port SSH cho ƒë√∫ng m√¥i tr∆∞·ªùng
sau n√†y th√™m server th√¨ nh·ªõ ch·∫°y l·∫°i c√°i n√†y l√† oke

```bash
SUBNET="192.168.0.0/24"
PORT=8022
USER="thang2k6adu"
MASTER_IP=$(hostname -I | awk '{print $1}')

mkdir -p ~/k3s-inventory && cd ~/k3s-inventory

echo -e "[master]\n$MASTER_IP ansible_user=$USER ansible_port=$PORT worker_ip=$MASTER_IP\n\n[workers]" > hosts.ini

sudo nmap -p $PORT --open $SUBNET \
| grep "Nmap scan report" \
| grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" \
| grep -v "$MASTER_IP" \
| sed "s/.*/& ansible_user=$USER ansible_port=$PORT worker_ip=&/" \
>> hosts.ini

cd ~/
```

Check file inventory:

```bash
cat ~/k3s-inventory/hosts.ini
```

K·∫øt qu·∫£ mong ƒë·ª£i:

```ini
[master]
192.168.0.104 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.104

[workers]
192.168.0.105 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.105
192.168.0.106 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.106
```

---

## B∆Ø·ªöC 4: C√ÄI K3S CONTROL PLANE (MASTER)

ƒê·∫∑t t√™n node l√† `k3s-master`:

```bash
curl -sfL https://get.k3s.io | sh -s - \
  --write-kubeconfig-mode 644 \
  --node-name k3s-master
```

Check:

```bash
kubectl get nodes
```

---

## B∆Ø·ªöC 5: M·ªû FIREWALL (UFW)

### Master:

```bash
sudo ufw allow 6443/tcp   # worker k·∫øt n·ªëi v·ªÅ master
sudo ufw allow 8472/udp   # pod giao ti·∫øp
sudo ufw allow 10250/tcp  # l·∫•y log pod
```

### Worker (b·∫±ng Ansible):

```bash
sudo ufw allow 8472/udp
sudo ufw allow 10250/tcp
```

---

## C√ÄI ANSIBLE TR√äN MASTER

```bash
sudo apt update
sudo apt install ansible -y
```

Test k·∫øt n·ªëi:

```bash
ansible all -i ~/k3s-inventory/hosts.ini -m ping
```

---

## SET SUDO KH√îNG PASSWORD (CHO WORKER)

T·∫°o file:

```bash
nano ~/k3s-inventory/setup-sudo.yml
```

```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow thang2k6adu sudo without password
      copy:
        dest: /etc/sudoers.d/thang2k6adu
        content: |
          thang2k6adu ALL=(ALL) NOPASSWD:ALL
        owner: root
        group: root
        mode: '0440'
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/setup-sudo.yml -K
```

---

## SET IP Tƒ®NH CHO WORKER (OPTIONAL)

```bash
nano ~/k3s-inventory/set-static-ip.yml
```

```yaml
- hosts: workers
  become: yes
  vars:
    dns:
      - 8.8.8.8
      - 1.1.1.1

  tasks:
    - name: Disable cloud-init network
      copy:
        dest: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
        content: |
          network: {config: disabled}

    - name: Remove old netplan config
      file:
        path: /etc/netplan/50-cloud-init.yaml
        state: absent

    - name: Configure static IP
      copy:
        dest: /etc/netplan/01-static.yaml
        content: |
          network:
            version: 2
            renderer: networkd
            ethernets:
              {{ ansible_default_ipv4.interface }}:
                dhcp4: no
                addresses:
                  - {{ hostvars[inventory_hostname].worker_ip }}/24
                gateway4: {{ ansible_default_ipv4.gateway }}
                nameservers:
                  addresses:
                    {% for d in dns %}
                    - {{ d }}
                    {% endfor %}

    - name: Apply netplan
      command: netplan apply
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/set-static-ip.yml
```

Check:

```bash
ansible workers -i ~/k3s-inventory/hosts.ini -m shell -a \
"echo '=== HOST:' \$(hostname) && ip a | grep inet && ip route | grep default && ping -c 2 8.8.8.8"
```

---

## L·∫§Y TOKEN T·ª™ MASTER

```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

V√≠ d·ª•:

```
K10a3f9c8c7b2a3b7f9::server:xxxxxxxx
```

---

## M·ªû FIREWALL CHO WORKER (ANSIBLE)

```bash
nano ~/k3s-inventory/open-ufw-worker.yml
```

```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow flannel VXLAN (8472/udp)
      ufw:
        rule: allow
        port: 8472
        proto: udp

    - name: Allow kubelet API (10250/tcp)
      ufw:
        rule: allow
        port: 10250
        proto: tcp

    - name: Enable UFW
      ufw:
        state: enabled
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/open-ufw-worker.yml
```

---

## C√ÄI K3S AGENT (WORKER)

```bash
nano ~/k3s-inventory/install-k3s-worker.yml
```

```yaml
- hosts: workers
  become: yes
  vars:
    k3s_url: "https://192.168.0.104:6443"
    k3s_token: "K1028f433ac447753d1b7936476a06b90846c8d7d8ff9d3b54232c88638075cdb87::server:ed9d1a2aa018911c52cf9f100e36a157"

  tasks:
    - name: Install k3s agent
      shell: |
        curl -sfL https://get.k3s.io | K3S_URL={{ k3s_url }} K3S_TOKEN={{ k3s_token }} sh -
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/install-k3s-worker.yml
```

---

## CHECK NODE ƒê√É JOIN

```bash
kubectl get nodes -o wide
```

Output:

```
NAME         STATUS   ROLES           IP
k3s-master   Ready    control-plane   192.168.0.104
worker1      Ready    <none>           192.168.0.105
worker2      Ready    <none>           192.168.0.106
```

---

## SET ROLE CHO WORKER

```bash
kubectl get nodes --no-headers | awk '{print $1}' | grep -v master | xargs -I {} kubectl label node {} node-role.kubernetes.io/worker=worker
```

Check:

```bash
kubectl get nodes
```

Output:

```
NAME            STATUS   ROLES    AGE
192.168.0.105   Ready    worker   1d
192.168.0.106   Ready    worker   1d
```

# üöÄ C√ÄI HELM + KUBERNETES DASHBOARD

## 1Ô∏è‚É£ C√†i Helm

```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version
```

---

## 2Ô∏è‚É£ C√†i Kubernetes Dashboard

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
```

Check:

```bash
kubectl get pods -n kubernetes-dashboard
```

---

## 3Ô∏è‚É£ T·∫°o ServiceAccount (t√†i kho·∫£n cho service)

T·∫°o file:

```bash
nano ~/k3s-inventory/dashboard-admin.yaml
```

N·ªôi dung:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubernetes-dashboard-admin
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard-admin
  namespace: kubernetes-dashboard
```

Apply:

```bash
kubectl apply -f ~/k3s-inventory/dashboard-admin.yaml
```

Check service:

```bash
kubectl get svc -n kubernetes-dashboard
```

---

## 4Ô∏è‚É£ M·ªü proxy ƒë·ªÉ truy c·∫≠p Dashboard

```bash
kubectl proxy --address=0.0.0.0 --accept-hosts='^.*$'
```

N·∫øu kh√¥ng m·ªü proxy t·∫°i port `8001` th√¨ ph·∫£i v√†o `6443` (ch·∫Øc ch·∫Øn kh√¥ng v√†o ƒë∆∞·ª£c).

Truy c·∫≠p Dashboard:

```
http://192.168.0.104:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
```

Gi·∫£i th√≠ch:

> ‚ÄúAPI Server, h√£y forward request n√†y t·ªõi Service kubernetes-dashboard, port t√™n l√† https (443), n√≥ l√† port‚Äù

---

## 5Ô∏è‚É£ L·∫•y token ƒë·ªÉ login Dashboard

```bash
kubectl -n kubernetes-dashboard create token kubernetes-dashboard-admin
```

---

## 6Ô∏è‚É£ N·∫øu SSH th√¨ t·∫°m m·ªü port 8001

```bash
sudo ufw allow 8001
sudo ufw reload
```

Sau khi d√πng xong th√¨ ƒë√≥ng l·∫°i:

```bash
sudo ufw delete allow 8001
sudo ufw reload
```
T·∫•t c·∫£ pod ·ªü node n√†o?
kubectl get pods -A -o wide

---


test deploy nginx + node port

kubectl create namespace test-nginx

<!-- L·ªánh n√†y t·∫°o deployment tr√™n node b·∫•t k√¨ (schedule t·ª± ch·ªçn t·ªëi ∆∞u) -->
kubectl create deployment nginx \
  --image=nginx \
  -n test-nginx

check
kubectl get pods -n test-nginx


expose

<!-- N√†y gi·ªëng t·∫°o 1 service port 80, node port b·∫•t k√¨ tr·ªè v·ªÅ nginx
n√≥ s·∫Ω m·ªü port c·ªßa t·∫•t c·∫£ c√°c node

yaml ph·∫£i type node port, ko l√† n√≥ v·ªÅ ClusterIP
 -->
kubectl expose deployment nginx \
  --type=NodePort \
  --port=80 \
  -n test-nginx

check
kubectl get svc -n test-nginx

nginx   NodePort   10.43.7.190   <none>        80:30582/TCP   11s

v√†o
http://192.168.0.105:30582

scale th·ª≠

kubectl scale deployment -n test-nginx nginx --replicas=3
kubectl get pods -n test-nginx -o wide


rollback
kubectl delete namespace test-nginx

setup ingress (ko c·∫ßn nodeport n·ªØa)

gh√©t traefik n√™n disable ƒëi

sudo nano /etc/rancher/k3s/config.yaml

disable:
  - traefik

sudo systemctl restart k3s

check
kubectl get pods -n kube-system

c√†i nginx

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

kubectl create namespace ingress-nginx

c√°i n√†y cho reverse proxy, c√≤n cloud c√≥ LB s·∫µn n√™n l√† kh√°c

mkdir -p ~/k3s-inventory/nginx-ingress-config
nano ~/k3s-inventory/nginx-ingress-config/values.yaml

controller:
  replicaCount: 2

  ingressClassResource:
    enabled: true
    default: true
    name: nginx

  kind: Deployment

  service:
    enabled: true
    type: NodePort
    externalTrafficPolicy: Local
    ports:
      http: 80
      https: 443
    nodePorts:
      http: 30080
      https: 30443

  resources:
    requests:
      cpu: 200m
      memory: 256Mi

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
    targetCPUUtilizationPercentage: 60

  config:
    use-forwarded-headers: "true"
    proxy-real-ip-cidr: "0.0.0.0/0"
    real-ip-header: "X-Forwarded-For"
    proxy-body-size: "50m"
    proxy-read-timeout: "600"
    proxy-send-timeout: "600"
    worker-shutdown-timeout: "240s"
    enable-underscores-in-headers: "true"

  allowSnippetAnnotations: false

  metrics:
    enabled: true
    service:
      enabled: true
    serviceMonitor:
      enabled: true

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                  - controller
          topologyKey: kubernetes.io/hostname

  terminationGracePeriodSeconds: 300

  lifecycle:
    preStop:
      exec:
        command:
          - /wait-shutdown

defaultBackend:
  enabled: true


helm install ingress-nginx ingress-nginx/ingress-nginx \
  -n ingress-nginx \
  -f ~/k3s-inventory/nginx-ingress-config/values.yaml

check
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

l√†m l·∫°i nh∆∞ c≈©, kh√°c l√† service l√∫c n√†y l√† Cluster IP ch·ª© ko d√πng node port

kubectl create namespace test-nginx

kubectl create deployment nginx \
  --image=nginx \
  -n test-nginx

kh√°c n√® (kh√¥ng ghi type th√¨ l√† ClusterIP), ko name th√¨ c√πng t√™n v·ªõi deployment
ko ƒë·ªãnh nghƒ©a target port th√¨ t·ª± l·∫•y trong deployment

kubectl expose deployment nginx \
  --port=80 \
  --target-port=80 \
  -n test-nginx

kubectl get svc -n test-nginx

mkdir ~/k8s-manifest
nano ~/k8s-manifest/nginx-ingress.yaml

prefix s·∫Ω match v·ªõi t·∫•t c·∫£

http://nginx.local/
http://nginx.local/abc
http://nginx.local/api
http://nginx.local/test/123

ƒë·ªÅu v√†o nginx h·∫øt

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: test-nginx
spec:
  rules:
  - host: nginx.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80

kubectl apply -f ~/k8s-manifest/nginx-ingress.yaml

check

kubectl get ingress -n test-nginx

map domain v√†o dns ·ªü host

V√≠ d·ª• window, c√≤n linux kh√° d·ªÖ th√¥i

ch·∫°y power shell b·∫±ng admin

notepad C:\Windows\System32\drivers\etc\hosts

flush dns (x√≥a cache)
ipconfig /flushdns

ping th·ª≠ ph√°t
ping nginx.local

sudo ufw allow 80
sudo ufw allow 443



v√†o
http://nginx.local

