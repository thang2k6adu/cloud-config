# üöÄ H∆Ø·ªöNG D·∫™N TRI·ªÇN KHAI K3S CLUSTER (MASTER + WORKER)

---

## B∆Ø·ªöC 1: ƒê·ªîI HOSTNAME (TR√äN NODE MASTER)

> Nh·ªõ d√πng `ip a` ƒë·ªÉ check **IP / mask / gateway** v√† thay cho ƒë√∫ng tr∆∞·ªõc khi l√†m b·∫•t c·ª© ƒëi·ªÅu g√¨.

```bash
sudo hostnamectl set-hostname k3s-master
sudo nano /etc/hosts
```

V√≠ d·ª• n·ªôi dung:

```txt
127.0.0.1 localhost
192.168.0.104 k3s-master
```

Reboot:

```bash
sudo reboot
```

---

## B∆Ø·ªöC 2: SET IP Tƒ®NH + DISABLE CLOUD-INIT (MASTER)

Disable cloud-init network:

```bash
sudo nano /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
```

N·ªôi dung:

```yaml
network: {config: disabled}
```

X√≥a netplan c≈©:

```bash
sudo rm -f /etc/netplan/50-cloud-init.yaml
```

T·∫°o netplan m·ªõi:

```bash
sudo nano /etc/netplan/01-static.yaml
```

N·ªôi dung:

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    ens33:
      dhcp4: no
      addresses:
        - 192.168.0.104/24
      gateway4: 192.168.0.1
      nameservers:
        addresses:
          - 8.8.8.8
          - 1.1.1.1
```

Apply:

```bash
sudo netplan apply
```

Check IP:

```bash
ip a
```

---

## B∆Ø·ªöC 3: SCAN IP C√ÅC SERVER WORKER (TR√äN MASTER)

C√†i `nmap`:

```bash
sudo apt install nmap -y
```

Auto generate inventory file

> ‚ö†Ô∏è Nh·ªõ s·ª≠a subnet + port SSH cho ƒë√∫ng m√¥i tr∆∞·ªùng
sau n√†y th√™m server th√¨ nh·ªõ ch·∫°y l·∫°i c√°i n√†y l√† oke

```bash
SUBNET="192.168.0.0/24"
PORT=8022
USER="thang2k6adu"
MASTER_IP=$(hostname -I | awk '{print $1}')

mkdir -p ~/k3s-inventory && cd ~/k3s-inventory

echo -e "[master]\n$MASTER_IP ansible_user=$USER ansible_port=$PORT worker_ip=$MASTER_IP\n\n[workers]" > hosts.ini

sudo nmap -p $PORT --open $SUBNET \
| grep "Nmap scan report" \
| grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" \
| grep -v "$MASTER_IP" \
| sed "s/.*/& ansible_user=$USER ansible_port=$PORT worker_ip=&/" \
>> hosts.ini

cd ~/
```

Check file inventory:

```bash
cat ~/k3s-inventory/hosts.ini
```

K·∫øt qu·∫£ mong ƒë·ª£i:

```ini
[master]
192.168.0.104 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.104

[workers]
192.168.0.105 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.105
192.168.0.106 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.106
```

---

## B∆Ø·ªöC 4: C√ÄI K3S CONTROL PLANE (MASTER)

ƒê·∫∑t t√™n node l√† `k3s-master`:

```bash
curl -sfL https://get.k3s.io | sh -s - \
  --write-kubeconfig-mode 644 \
  --node-name k3s-master
```

Check:

```bash
kubectl get nodes
```

---

## B∆Ø·ªöC 5: M·ªû FIREWALL (UFW)

### Master:

```bash
sudo ufw allow 6443/tcp   # worker k·∫øt n·ªëi v·ªÅ master
sudo ufw allow 8472/udp   # pod giao ti·∫øp
sudo ufw allow 10250/tcp  # l·∫•y log pod
```

### Worker (b·∫±ng Ansible):

```bash
sudo ufw allow 8472/udp
sudo ufw allow 10250/tcp
```

---

## C√ÄI ANSIBLE TR√äN MASTER

```bash
sudo apt update
sudo apt install ansible -y
```

Test k·∫øt n·ªëi:

```bash
ansible all -i ~/k3s-inventory/hosts.ini -m ping
```

---

## SET SUDO KH√îNG PASSWORD (CHO WORKER)

T·∫°o file:

```bash
nano ~/k3s-inventory/setup-sudo.yml
```

```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow thang2k6adu sudo without password
      copy:
        dest: /etc/sudoers.d/thang2k6adu
        content: |
          thang2k6adu ALL=(ALL) NOPASSWD:ALL
        owner: root
        group: root
        mode: '0440'
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/setup-sudo.yml -K
```

---

## SET IP Tƒ®NH CHO WORKER (OPTIONAL)

```bash
nano ~/k3s-inventory/set-static-ip.yml
```

```yaml
- hosts: workers
  become: yes
  vars:
    dns:
      - 8.8.8.8
      - 1.1.1.1

  tasks:
    - name: Disable cloud-init network
      copy:
        dest: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
        content: |
          network: {config: disabled}

    - name: Remove old netplan config
      file:
        path: /etc/netplan/50-cloud-init.yaml
        state: absent

    - name: Configure static IP
      copy:
        dest: /etc/netplan/01-static.yaml
        content: |
          network:
            version: 2
            renderer: networkd
            ethernets:
              {{ ansible_default_ipv4.interface }}:
                dhcp4: no
                addresses:
                  - {{ hostvars[inventory_hostname].worker_ip }}/24
                gateway4: {{ ansible_default_ipv4.gateway }}
                nameservers:
                  addresses:
                    {% for d in dns %}
                    - {{ d }}
                    {% endfor %}

    - name: Apply netplan
      command: netplan apply
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/set-static-ip.yml
```

Check:

```bash
ansible workers -i ~/k3s-inventory/hosts.ini -m shell -a \
"echo '=== HOST:' \$(hostname) && ip a | grep inet && ip route | grep default && ping -c 2 8.8.8.8"
```

---

## L·∫§Y TOKEN T·ª™ MASTER

```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

V√≠ d·ª•:

```
K10a3f9c8c7b2a3b7f9::server:xxxxxxxx
```

---

## M·ªû FIREWALL CHO WORKER (ANSIBLE)

```bash
nano ~/k3s-inventory/open-ufw-worker.yml
```

```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow flannel VXLAN (8472/udp)
      ufw:
        rule: allow
        port: 8472
        proto: udp

    - name: Allow kubelet API (10250/tcp)
      ufw:
        rule: allow
        port: 10250
        proto: tcp

    - name: Enable UFW
      ufw:
        state: enabled
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/open-ufw-worker.yml
```

---

## C√ÄI K3S AGENT (WORKER)

```bash
nano ~/k3s-inventory/install-k3s-worker.yml
```

```yaml
- hosts: workers
  become: yes
  vars:
    k3s_url: "https://192.168.0.104:6443"
    k3s_token: "K1028f433ac447753d1b7936476a06b90846c8d7d8ff9d3b54232c88638075cdb87::server:ed9d1a2aa018911c52cf9f100e36a157"

  tasks:
    - name: Install k3s agent
      shell: |
        curl -sfL https://get.k3s.io | K3S_URL={{ k3s_url }} K3S_TOKEN={{ k3s_token }} sh -
```

Run:

```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/install-k3s-worker.yml
```

---

## CHECK NODE ƒê√É JOIN

```bash
kubectl get nodes -o wide
```

Output:

```
NAME         STATUS   ROLES           IP
k3s-master   Ready    control-plane   192.168.0.104
worker1      Ready    <none>           192.168.0.105
worker2      Ready    <none>           192.168.0.106
```

---

## SET ROLE CHO WORKER

```bash
kubectl get nodes --no-headers | awk '{print $1}' | grep -v master | xargs -I {} kubectl label node {} node-role.kubernetes.io/worker=worker
```

Check:

```bash
kubectl get nodes
```

Output:

```
NAME            STATUS   ROLES    AGE
192.168.0.105   Ready    worker   1d
192.168.0.106   Ready    worker   1d
```

---

c√†i helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

check
kubectl get pods -n kubernetes-dashboard

t·∫°o ServiceAccount (t√†i kho·∫£n cho service)
nano ~/k3s-inventory/dashboard-admin.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubernetes-dashboard-admin
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard-admin
  namespace: kubernetes-dashboard

kubectl apply -f ~/k3s-inventory/dashboard-admin.yaml

check
kubectl get svc -n kubernetes-dashboard


kubectl proxy --address=0.0.0.0 --accept-hosts='^.*$'

n·∫øu ko m·ªü proxy t·∫°i port 8001 th√¨ ph·∫£i v√†o 6443 (ch·∫Øc ch·∫Øn ko v√†o ƒë∆∞·ª£c)

http://192.168.0.104:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

‚ÄúAPI Server, h√£y forward request n√†y t·ªõi Service kubernetes-dashboard, port t√™n l√† https (443), n√≥ l√† port‚Äù

l·∫•y token (ƒë·ªÉ d√πng m·ªói l·∫ßn login)
kubectl -n kubernetes-dashboard create token kubernetes-dashboard-admin

<!-- N·∫øu ssh th√¨ t·∫°m m·ªü port 8001 -->
sudo ufw allow 8001
sudo ufw reload

sudo ufw delete allow 8001
sudo ufw reload
